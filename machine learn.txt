7. Use the concept of Data Augmentation to increase the data size
from a single image.
import cv2
import numpy as np
import albumentations as A
import os
def augment_image(image_path, output_folder, num_augmented=10):
"""
Applies data augmentation techniques to generate multiple variations of an image.
Parameters:- image_path: Path to the input image.- output_folder: Folder where augmented images will be saved.- num_augmented: Number of augmented images to generate.
"""
if not os.path.exists(output_folder):
os.makedirs(output_folder)
# The problem was here: ’ss.jpeg’ needed to be a string
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Also, you should convert the loaded ’image’
transform = A.Compose([
A.HorizontalFlip(p=0.5),
A.VerticalFlip(p=0.3),
A.Rotate(limit=30, p=0.5),
A.RandomBrightnessContrast(p=0.5),
A.GaussianBlur(blur_limit=(3, 7), p=0.3),
A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
])
for i in range(num_augmented):
augmented = transform(image=image)
augmented_image = augmented["image"]
save_path = os.path.join(output_folder, f"augmented_{i}.jpg")
cv2.imwrite(save_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))
print(f"{num_augmented} augmented images saved in {output_folder}")
# Example Usage
image_path = "ss.png" # Change this to the actual image path
output_folder = "augmented_images"
augment_image(image_path, output_folder, num_augmented=10)
Output:
10 augmented images saved in augmented_images
<ipython-input-6-548755f45b14>:28: UserWarning: Argument(s) ’var_limit’ are not valid for transform GaussNoise
14

9. Design and implement a CNN model to classify CIFAR10 im
age dataset. Use Data Augmentation while designing the CNN
model. Record accuracy corresponding to the number of epochs.
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
# Load MNIST and Fashion-MNIST datasets
datasets = {
"MNIST": tf.keras.datasets.mnist.load_data(),
"Fashion-MNIST": tf.keras.datasets.fashion_mnist.load_data()
}
for dataset_name, (data_train, data_test) in datasets.items():
(x_train, y_train), (x_test, y_test) = data_train, data_test
# Normalize pixel values (0 to 255-> 0 to 1)
x_train, x_test = x_train / 255.0, x_test / 255.0
# Reshape data to fit CNN input format (28x28 images with 1 channel)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)
# Convert labels to categorical format (one-hot encoding)
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)
# Define LeNet-5 model
model = models.Sequential([
layers.Conv2D(6, (5, 5), activation=’tanh’, padding=’same’, input_shape=(28, 28, 1)),
layers.AveragePooling2D((2, 2)),
layers.Conv2D(16, (5, 5), activation=’tanh’),
layers.AveragePooling2D((2, 2)),
layers.Flatten(),
layers.Dense(120, activation=’tanh’),
layers.Dense(84, activation=’tanh’),
layers.Dense(10, activation=’softmax’) # Output layer with 10 classes
])
# Compile the model
model.compile(optimizer=’adam’,
loss=’categorical_crossentropy’,
metrics=[’accuracy’])
# Train the model
history = model.fit(x_train, y_train, epochs=15, batch_size=128,
validation_data=(x_test, y_test), verbose=1)
# Evaluate accuracy
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\n{dataset_name} Test Accuracy: {test_acc:.4f}\n")
17
# Plot accuracy vs. epochs
plt.plot(history.history[’accuracy’], label=’Training Accuracy’)
plt.plot(history.history[’val_accuracy’], label=’Validation Accuracy’)
plt.xlabel(’Epochs’)
plt.ylabel(’Accuracy’)
plt.legend()
plt.title(f’LeNet-5 Accuracy on {dataset_name}’

10. ImplementthestandardVGG-16&VGG-19CNNarchitecture
modelstoclassifymulti-categoryimagedatasetsandcheckthe accuracy.
import tensorflowastf
from tensorflow.kerasimportlayers,models
from tensorflow.keras.applicationsimportVGG16,VGG19
from tensorflow.keras.preprocessing.imageimportImageDataGenerator
import matplotlib.pyplotasplt
#Load CIFAR-10dataset(canbereplacedwithMNIST/Fashion-MNIST)
(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()
#Normalizepixelvalues(0to255->0to1)
x_train,x_test=x_train/255.0,x_test/255.0
#Convertlabelstocategoricalformat
y_train=tf.keras.utils.to_categorical(y_train,10)
y_test =tf.keras.utils.to_categorical(y_test,10)
19
# Data Augmentation
datagen = ImageDataGenerator(
rotation_range=15,
width_shift_range=0.1,
height_shift_range=0.1,
horizontal_flip=True,
zoom_range=0.1
)
datagen.fit(x_train)
# Function to build VGG model
def build_vgg_model(vgg_type="VGG16"):
base_model = VGG16(weights=None, include_top=False, input_shape=(32, 32, 3)) if vgg_type == "VGG16" \
else VGG19(weights=None, include_top=False, input_shape=(32, 32, 3))
model = models.Sequential([
base_model,
layers.Flatten(),
layers.Dense(512, activation=’relu’),
layers.Dropout(0.5),
layers.Dense(10, activation=’softmax’)
])
model.compile(optimizer=’adam’,
loss=’categorical_crossentropy’,
metrics=[’accuracy’])
return model
# Train and evaluate both VGG models
for vgg_type in ["VGG16", "VGG19"]:
print(f"\nTraining {vgg_type} Model...\n")
model = build_vgg_model(vgg_type)
history = model.fit(datagen.flow(x_train, y_train, batch_size=64),
validation_data=(x_test, y_test),
epochs=20, verbose=1)
# Evaluate test accuracy
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\n{vgg_type} Test Accuracy: {test_acc:.4f}\n")
# Plot accuracy vs. epochs
plt.plot(history.history[’accuracy’], label=’Training Accuracy’)
plt.plot(history.history[’val_accuracy’], label=’Validation Accuracy’)
plt.xlabel(’Epochs’)
plt.ylabel(’Accuracy’)
plt.legend()
plt.title(f’Accuracy of {vgg_type} on CIFAR-10’)
plt.show()
A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),

11. Implement RNN for sentiment analysis on movie reviews.
21
import tensorflow as tf
from tensorflow.keras import layers, models, preprocessing
import matplotlib.pyplot as plt
# Load IMDb dataset
vocab_size = 10000 # Limit vocabulary size
max_length = 200 # Pad sequences to this length
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)
# Pad sequences to ensure uniform length
x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)
# Define RNN model for sentiment analysis
model = models.Sequential([
layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),
layers.SimpleRNN(64, activation=’tanh’, return_sequences=True),
layers.SimpleRNN(32, activation=’tanh’),
layers.Dense(1, activation=’sigmoid’) # Binary classification (positive/negative sentiment)
])
# Compile the model
model.compile(optimizer=’adam’,
loss=’binary_crossentropy’,
metrics=[’accuracy’])
# Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=64,
validation_data=(x_test, y_test), verbose=1)
# Evaluate model performance
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\nTest Accuracy: {test_acc:.4f}\n")
# Plot accuracy vs. epochs
plt.plot(history.history[’accuracy’], label=’Training Accuracy’)
plt.plot(history.history[’val_accuracy’], label=’Validation Accuracy’)
plt.xlabel(’Epochs’)
plt.ylabel(’Accuracy’)
plt.legend()
plt.title(’RNN Accuracy on IMDb Sentiment Analysis’)
plt.show()

12. ImplementBidirectionalLSTMforsentimentanalysisonmovie
reviews.
import tensorflowastf
from tensorflow.kerasimportlayers,models,preprocessing
import matplotlib.pyplotasplt
#Load IMDbdataset
vocab_size=10000 #Limitvocabularysize
max_length=200 #Padsequencestothislength
(x_train,y_train),(x_test,y_test)=tf.keras.datasets.imdb.load_data(num_words=vocab_size)
#Padsequencestoensureuniformlength
x_train=preprocessing.sequence.pad_sequences(x_train,maxlen=max_length)
x_test =preprocessing.sequence.pad_sequences(x_test,maxlen=max_length)
#DefineBidirectionalLSTMmodel
model=models.Sequential([
layers.Embedding(input_dim=vocab_size,output_dim=128,input_length=max_length),
layers.Bidirectional(layers.LSTM(64,return_sequences=True)), #FirstLSTMlayer
layers.Bidirectional(layers.LSTM(32)), #SecondLSTMlayer
layers.Dense(1,activation=’sigmoid’) #Binaryclassification(positive/negativesentiment)
])
#Compilethemodel
model.compile(optimizer=’adam’,
23
loss=’binary_crossentropy’,
metrics=[’accuracy’])
#Trainthemodel
history=model.fit(x_train,y_train,epochs=10,batch_size=64,
validation_data=(x_test,y_test),verbose=1)
#Evaluatemodelperformance
test_loss,test_acc=model.evaluate(x_test,y_test,verbose=2)
print(f"\nTestAccuracy:{test_acc:.4f}\n")
#Plotaccuracyvs.epochs
plt.plot(history.history[’accuracy’],label=’TrainingAccuracy’)
plt.plot(history.history[’val_accuracy’],label=’ValidationAccuracy’)
plt.xlabel(’Epochs’)
plt.ylabel(’Accuracy’)
plt.legend()
plt.title(’BidirectionalLSTMAccuracyonIMDbSentimentAnalysis’)
plt.show()
