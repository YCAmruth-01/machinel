PROGRAM 1
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Generate a synthetic linearly separable dataset
X, y = make_classification(
    n_samples=100, n_features=2, n_informative=2,
    n_redundant=0, n_clusters_per_class=1,
    flip_y=0, random_state=42
)

# Split the dataset into training and testing subsets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train and evaluate the Perceptron
perceptron = Perceptron(max_iter=1000, eta0=1, random_state=42)
perceptron.fit(X_train, y_train)
accuracy = accuracy_score(y_test, perceptron.predict(X_test))
print(f'Accuracy: {accuracy:.2f}')

# Plot decision boundary
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),
    np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100)
)
Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='coolwarm', marker='o')
plt.title('Perceptron Classification')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

PROGRAM 2
import numpy as np
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score
# Data for AND, OR, XOR gates
data = {
    'AND': (np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1])),
    # Use 0 and 1 for labels
    'OR': (np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 1])),
    # Use 0 and 1 for labels
    'XOR': (np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0])),
    # Use 0 and 1 for labels
}
# Classify AND, OR, XOR gates
for gate, (X, y) in data.items():
    # Initialize and train the Perceptron
    perceptron = Perceptron(max_iter=10, eta0=1, random_state=42)
    perceptron.fit(X, y)
    # Make predictions
    y_pred = perceptron.predict(X)
    # Calculate accuracy
    acc = accuracy_score(y, y_pred) * 100
    print(f"{gate} gate accuracy: {acc:.2f}%")
    # Output predictions and true labels for clarity
    print(f"Predictions: {y_pred}")
    print(f"True Labels: {y}")

PROGRAM 3
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Load and preprocess the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
# One-hot encode labels
y = to_categorical(y)
# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)
# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# Build the neural network with exactly four hidden layers
model = Sequential([
Dense(64, activation='relu', input_shape=(X_train.shape[1],)), # Input layer
Dense(64, activation='relu'), # Hidden layer 1
Dense(64, activation='relu'), # Hidden layer 2
Dense(64, activation='relu'), # Hidden layer 3
Dense(64, activation='relu'), # Hidden layer 4
Dense(3, activation='softmax') # Output layer with 3 units for 3 classes
])
# Compile the model
model.compile(optimizer='adam',
loss='categorical_crossentropy',
metrics=['accuracy'])
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_split=0.2, verbose=1)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy * 100:.2f}%')
# Make predictions
predictions = model.predict(X_test)
predicted_classes = tf.argmax(predictions, axis=1)
true_classes = tf.argmax(y_test, axis=1)
accuracy = tf.reduce_mean(tf.cast(predicted_classes == true_classes, tf.float32))
print(f'Predicted Accuracy: {accuracy.numpy() * 100:.2f}%')

PROGRAM 4
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import numpy as np
iris = load_iris()
X = iris.data
y = iris.target
encoder = OneHotEncoder(sparse_output=False)
y = encoder.fit_transform(y.reshape(-1, 1))
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
class DeepNN(tf.keras.Model):
def __init__(self):
super(DeepNN, self).__init__()
self.dense1 = tf.keras.layers.Dense(64, activation=’relu’)
self.dense2 = tf.keras.layers.Dense(64, activation=’relu’)
self.dense3 = tf.keras.layers.Dense(64, activation=’relu’)
self.dense4 = tf.keras.layers.Dense(64, activation=’relu’)
self.output_layer = tf.keras.layers.Dense(3, activation=’softmax’)
def call(self, inputs):
x = self.dense1(inputs)
x = self.dense2(x)
x = self.dense3(x)
x = self.dense4(x)
return self.output_layer(x)
model = DeepNN()
def compute_loss(y_true, y_pred):
return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))
optimizer = tf.keras.optimizers.Adam()
def train_step(X_batch, y_batch):
with tf.GradientTape() as tape:
y_pred = model(X_batch, training=True)
loss = compute_loss(y_batch, y_pred)
gradients = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
return loss
epochs = 100
batch_size = 10
for epoch in range(epochs):
for i in range(0, len(X_train), batch_size):
9
X_batch = X_train[i:i+batch_size]
y_batch = y_train[i:i+batch_size]
loss = train_step(X_batch, y_batch)
if epoch % 10 == 0:
print(f’Epoch {epoch}, Loss: {loss.numpy()}’)
def evaluate_model(X_test, y_test):
y_pred = model(X_test, training=False)
correct_predictions = tf.equal(tf.argmax(y_test, axis=1), tf.argmax(y_pred, axis=1))
accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))
return accuracy.numpy()
accuracy = evaluate_model(X_test, y_test)
print(f’Test Accuracy: {accuracy * 100:.2f}%’)

PROGRAM 5
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
# Load and preprocess the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()
# Normalize the input data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the model
model = Sequential([
Flatten(input_shape=(28, 28)), # Flatten the input images
Dense(128, activation='relu'), # First hidden layer
Dense(256, activation='relu'), # Second hidden layer
Dense(10, activation='softmax') # Output layer
])
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)
# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.2f}')
